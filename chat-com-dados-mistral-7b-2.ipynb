{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccd8fdd7-e0ff-4b98-b919-cd22b6bc1923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T17:00:34.309752Z",
     "iopub.status.busy": "2024-02-19T17:00:34.309564Z",
     "iopub.status.idle": "2024-02-19T17:00:35.931410Z",
     "shell.execute_reply": "2024-02-19T17:00:35.931025Z",
     "shell.execute_reply.started": "2024-02-19T17:00:34.309734Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, PodSpec\n",
    "from langchain.vectorstores import Pinecone as PineconeStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain import HuggingFaceHub\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d14191b-393f-43bb-9e5f-37a0240bc493",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T17:00:35.932595Z",
     "iopub.status.busy": "2024-02-19T17:00:35.932095Z",
     "iopub.status.idle": "2024-02-19T17:00:35.936005Z",
     "shell.execute_reply": "2024-02-19T17:00:35.935776Z",
     "shell.execute_reply.started": "2024-02-19T17:00:35.932579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_dir=\"cache_dir\"\n",
    "load_dotenv('./.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c74eac80-e888-49e3-95d9-20149dd2ba2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T17:00:35.936478Z",
     "iopub.status.busy": "2024-02-19T17:00:35.936362Z",
     "iopub.status.idle": "2024-02-19T17:00:36.020151Z",
     "shell.execute_reply": "2024-02-19T17:00:36.019815Z",
     "shell.execute_reply.started": "2024-02-19T17:00:35.936469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available: 1\n",
      "GPU 0: NVIDIA RTX A4000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "if num_gpus > 0:\n",
    "    print(f\"GPUs available: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"GPU {i}: {gpu_name}\")\n",
    "else:\n",
    "    print(\"There is no GPU available. Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167eb948-647b-4c49-9fe1-6a8f2845b024",
   "metadata": {},
   "source": [
    "#### Leitura do arquivo e criação do splitter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a1cafe3-f4a4-4ef0-8ed9-ac39e3accd72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T17:00:36.020708Z",
     "iopub.status.busy": "2024-02-19T17:00:36.020548Z",
     "iopub.status.idle": "2024-02-19T17:00:36.022755Z",
     "shell.execute_reply": "2024-02-19T17:00:36.022469Z",
     "shell.execute_reply.started": "2024-02-19T17:00:36.020697Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('docs/nat.txt') as f:\n",
    "    clt = f.read()\n",
    "\n",
    "# Criação de um objeto RecursiveCharacterTextSplitter para dividir o texto em pedaços\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ad2f45e-6475-4e0b-8ad9-7b7fb0a9d8df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T17:00:36.023245Z",
     "iopub.status.busy": "2024-02-19T17:00:36.023119Z",
     "iopub.status.idle": "2024-02-19T17:00:36.031911Z",
     "shell.execute_reply": "2024-02-19T17:00:36.031630Z",
     "shell.execute_reply.started": "2024-02-19T17:00:36.023235Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chunks = text_splitter.create_documents([clt])\n",
    "text_chunks = [doc.page_content for doc in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d53798a5-5317-4fbc-ae1b-e06eca25e039",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T17:00:36.032424Z",
     "iopub.status.busy": "2024-02-19T17:00:36.032288Z",
     "iopub.status.idle": "2024-02-19T17:00:36.774574Z",
     "shell.execute_reply": "2024-02-19T17:00:36.774215Z",
     "shell.execute_reply.started": "2024-02-19T17:00:36.032414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1536,)\n"
     ]
    }
   ],
   "source": [
    "# Inicialize o modelo de embedding\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Isso retornará uma lista de vetores\n",
    "embeddings = model.encode(text_chunks, convert_to_tensor=False)  \n",
    "\n",
    "# Adiciona zeros para estender cada vetor até a dimensão 1536\n",
    "extended_embeddings = np.array([np.pad(emb, (0, 1536 - len(emb)), 'constant') for emb in embeddings])\n",
    "print(extended_embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1356940-9eb5-44fb-8ad0-e8ad77f1db06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T17:00:36.775676Z",
     "iopub.status.busy": "2024-02-19T17:00:36.775554Z",
     "iopub.status.idle": "2024-02-19T17:00:37.209615Z",
     "shell.execute_reply": "2024-02-19T17:00:37.209200Z",
     "shell.execute_reply.started": "2024-02-19T17:00:36.775662Z"
    }
   },
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=os.environ.get('PINECONE_API_KEY'))\n",
    "indexes = pc.list_indexes()\n",
    "index_name = 'natgpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24984a1b-802a-4b6a-9503-754eb0d1c75d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T17:00:37.210158Z",
     "iopub.status.busy": "2024-02-19T17:00:37.210025Z",
     "iopub.status.idle": "2024-02-19T17:00:49.026135Z",
     "shell.execute_reply": "2024-02-19T17:00:49.025571Z",
     "shell.execute_reply.started": "2024-02-19T17:00:37.210148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index encontrada e apagada: natgpt\n",
      "Index natgpt criado\n"
     ]
    }
   ],
   "source": [
    "for i in indexes:\n",
    "    pc.delete_index(i['name'])\n",
    "    print('Index encontrada e apagada: ' + i['name'])\n",
    "\n",
    "if index_name not in pc.list_indexes():\n",
    "    pc.create_index(index_name, dimension=1536, metric='cosine', spec=PodSpec(environment=os.environ.get('PINECONE_ENV')))\n",
    "    print('Index '+index_name+' criado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c997369-fac0-4109-bbd6-560b3f81394d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T17:00:49.026652Z",
     "iopub.status.busy": "2024-02-19T17:00:49.026552Z",
     "iopub.status.idle": "2024-02-19T17:00:49.306115Z",
     "shell.execute_reply": "2024-02-19T17:00:49.305767Z",
     "shell.execute_reply.started": "2024-02-19T17:00:49.026642Z"
    }
   },
   "outputs": [],
   "source": [
    "# Conecta na index criada\n",
    "pc_index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "123deabf-4206-4bdc-aa29-eb884a6e3a30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T17:00:49.306669Z",
     "iopub.status.busy": "2024-02-19T17:00:49.306521Z",
     "iopub.status.idle": "2024-02-19T17:00:49.308987Z",
     "shell.execute_reply": "2024-02-19T17:00:49.308665Z",
     "shell.execute_reply.started": "2024-02-19T17:00:49.306658Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "documents_to_insert = []\n",
    "for i, embedding in enumerate(extended_embeddings):\n",
    "    doc_id = f\"{i}\"\n",
    "    documents_to_insert.append({\"id\": doc_id, \"values\": embedding.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "147f56bc-ca9e-4bdd-8270-3f9e275c22cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T17:00:49.309605Z",
     "iopub.status.busy": "2024-02-19T17:00:49.309365Z",
     "iopub.status.idle": "2024-02-19T17:01:10.119280Z",
     "shell.execute_reply": "2024-02-19T17:01:10.118744Z",
     "shell.execute_reply.started": "2024-02-19T17:00:49.309594Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Insere os documentos na index do Pinecone\n",
    "pc_index.upsert(vectors=documents_to_insert)\n",
    "\n",
    "# Aguarda 20 segundos para dar tempo de atualizar a Index no Pinecone\n",
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f37e5b5-e7a1-419c-9ddf-b10e90225304",
   "metadata": {},
   "source": [
    "### Busca por similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a1c3699-498d-4a3f-a203-b4e834ae6029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T17:01:10.119992Z",
     "iopub.status.busy": "2024-02-19T17:01:10.119849Z",
     "iopub.status.idle": "2024-02-19T17:01:10.845502Z",
     "shell.execute_reply": "2024-02-19T17:01:10.844989Z",
     "shell.execute_reply.started": "2024-02-19T17:01:10.119981Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# SentenceTransformer para gerar o embedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Inicialização do modelo para perguntas e respostas\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.5, \n",
    "        \"max_new_tokens\": 512,\n",
    "        \"debug\": False,\n",
    "        \"top_k\": 30,\n",
    "        \"top_p\": 0.9,\n",
    "        \"repetition_penalty\": 1.0,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a01c9c4a-e705-413f-a2ab-da28fff8f6b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T17:01:10.846071Z",
     "iopub.status.busy": "2024-02-19T17:01:10.845942Z",
     "iopub.status.idle": "2024-02-19T17:01:10.849466Z",
     "shell.execute_reply": "2024-02-19T17:01:10.848883Z",
     "shell.execute_reply.started": "2024-02-19T17:01:10.846060Z"
    }
   },
   "outputs": [],
   "source": [
    "def perguntar(prompt, top_k=3, debug=False):\n",
    "    # Gerando o vetor de consulta\n",
    "    query_vector = model.encode(prompt, convert_to_tensor=False)\n",
    "    \n",
    "    # Adiciona zeros para estender cada vetor até a dimensão 1536\n",
    "    padding_length = 1536 - len(query_vector)\n",
    "    padded_vector = np.pad(query_vector, (0, padding_length), 'constant')\n",
    "    print(padded_vector.shape)\n",
    "    \n",
    "    query_vector_list = padded_vector.tolist()\n",
    "    query_result = pc_index.query(vector=query_vector_list, top_k=top_k)\n",
    "    \n",
    "    contexto = []\n",
    "    for index, match in enumerate(query_result.matches):\n",
    "        # print(\"ID: \" + match.id)\n",
    "        # print(\"Score: \" + str(query_result.matches[index]['score']))\n",
    "        # print(\"Text Value: \" + text_chunks[int(match.id)])\n",
    "        # print('-' * 50)\n",
    "        contexto.append(text_chunks[int(match.id)])\n",
    "\n",
    "    context_texts = contexto\n",
    "\n",
    "    combined_context = \" \".join(contexto)\n",
    "\n",
    "    input_text = f\"[INST]Responda em Português: {prompt}.\\n\\nContexto: {contexto}[/INST]\"\n",
    "\n",
    "    response = llm(input_text)\n",
    "\n",
    "    if(debug == True):\n",
    "        response = response + \"\\n\\n\\nDebug\\n---------------------------------\\n\" + combined_context\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e38e3c8-74b0-4440-ba33-beab2697bc56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T17:02:52.847508Z",
     "iopub.status.busy": "2024-02-19T17:02:52.847335Z",
     "iopub.status.idle": "2024-02-19T17:02:53.318912Z",
     "shell.execute_reply": "2024-02-19T17:02:53.318524Z",
     "shell.execute_reply.started": "2024-02-19T17:02:52.847494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1536,)\n",
      " Os coordenadores do NAT são Marcela Cristina Ozório e Bernardo Fiterman Albano.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    perguntar(\n",
    "        prompt='Quem são os coordenadores do nat?', top_k=3, debug=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95ab25ff-2b9b-40b2-ba2b-73d64454542d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T17:02:56.439547Z",
     "iopub.status.busy": "2024-02-19T17:02:56.439338Z",
     "iopub.status.idle": "2024-02-19T17:02:56.903639Z",
     "shell.execute_reply": "2024-02-19T17:02:56.903187Z",
     "shell.execute_reply.started": "2024-02-19T17:02:56.439534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1536,)\n",
      " O site do NAT pode ser acessado por meio do endereço: https://nat.mpac.mp.br.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    perguntar(\n",
    "        prompt='Qual o site do nat?', top_k=3, debug=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "515f32f1-77c9-4805-a20f-12e3fae5866b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T17:03:45.043932Z",
     "iopub.status.busy": "2024-02-19T17:03:45.043676Z",
     "iopub.status.idle": "2024-02-19T17:03:46.161722Z",
     "shell.execute_reply": "2024-02-19T17:03:46.161374Z",
     "shell.execute_reply.started": "2024-02-19T17:03:45.043916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1536,)\n",
      " Os principais setores do NAT são: Coordenação de Desenvolvimento de Sistemas, Observatório Criminal, LAB, Observatório de Políticas Públicas, Técinco-Científica.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    perguntar(\n",
    "        prompt='Quais os principais setores do NAT', top_k=3, debug=False\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
